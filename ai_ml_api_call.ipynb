{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c3ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "# I have set my api key on linux with this following command in terminal.\n",
    "# export TOGETHER_API_KEY=xxxxxxxxxxxxxx\n",
    "# then started jupyter notebook through the same terminal\n",
    "API_key = os.environ.get(\"TOGETHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb940c",
   "metadata": {},
   "source": [
    "# example by meta for a llama image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75c37b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  The animal in the image is a llama, a domesticated mammal native to South America.\n",
      "\n",
      "**Diet and Nutrition**\n",
      "\n",
      "Llamas are herbivores and their diet typically consists of:\n",
      "\n",
      "* Grasses\n",
      "* Leaves\n",
      "* Fruits\n",
      "* Vegetables\n",
      "\n",
      "In the wild, they feed on the vegetation available in their habitat, which includes high-altitude grasslands, plateaus, and mountains. Domesticated llamas are often fed hay, grains, and other feed supplemented with minerals and vitamins.\n",
      "\n",
      "**Habitat and Native Range**\n",
      "\n",
      "Llamas are native to the Andean region of South America, including present-day:\n",
      "\n",
      "* Peru\n",
      "* Bolivia\n",
      "* Ecuador\n",
      "* Chile\n",
      "* Argentina\n",
      "\n",
      "They inhabit high-altitude areas, including:\n",
      "\n",
      "* Grasslands\n",
      "* Plateaus\n",
      "* Mountains\n",
      "\n",
      "**Connection to AI Model**\n",
      "\n",
      "The term \"Llama\" is also associated with an artificial intelligence (AI) model developed by Meta AI. This model is designed for natural language processing tasks, such as text generation, language translation, and question answering. It is trained on large datasets of text and can generate human-like responses to user input.\n",
      "\n",
      "**Key Characteristics of Llama AI Model**\n",
      "\n",
      "* Highly effective in generating coherent and engaging text\n",
      "* Useful for various applications, including chatbots, language translation systems, and content generation tools.\n"
     ]
    }
   ],
   "source": [
    "# client = Together(base_url=\"https://api.aimlapi.com/v1\", api_key=API_key)\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": \"What sort of animal is in this picture? What is its usual diet? What area is the animal native to? And isn’t there some AI model that’s related to the image?\",\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\n",
    "#                         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/LLama.jpg/444px-LLama.jpg?20050123205659\",\n",
    "#                     },\n",
    "#                 },\n",
    "#             ],\n",
    "#         }\n",
    "#     ],\n",
    "#     max_tokens=300,\n",
    "# )\n",
    "\n",
    "# print(\"Assistant: \", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0aec51",
   "metadata": {},
   "source": [
    "## I have written the prompts in .txt files to easily alter them again and again for testing.\n",
    "## Important to load the prompts\n",
    "## system prompt tell the model what we are trying to do.\n",
    "## user prompt is the prompt attached with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b090c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in optical character recognition (OCR) and document analysis. Your job is to extract all the relevant text from any image provided, with special attention to format, headings, sub-headings, and body text. Be precise, follow the instructions strictly and return only the text that is present in the image and is asked from you.\n",
      "\n",
      "--------------------------------------------------\n",
      "This is complete vertically stacked image of multiple consecutive ordered pages of an IELTS reading passage. Your task is to carefully analyze the image and extract all the text from it. The passages are long so thats why whole passage is given to you in a form of vertically stiched one picture.\n",
      "\n",
      "\n",
      "STRICT IMPORTANT NOTE: I NEED EXTRACTED DATA IN THE FORMAT THAT I AM SAYING TO YOU WITHOUT ANY EXPLANATIONS OR SUGGESTIONS FROM YOUR SIDE. I AM A PROGRAMMER AND I WILL USE YOUR OCR EXTRACTED TEXT IN MY APPLICATION. I NEED ONLY REQUIRED EXTRACTED DATA FROM YOU AND NOTHING ELSE.\n",
      "\n",
      "Some visual cues to assist you in this process:\n",
      "1- There are most probably instructions too for the students to read the passage(they are mostly in the start of the picture), i also need those instructions.\n",
      "2- The largest and boldest text at the top of the picture is likely the title of the passage.\n",
      "3- If there is a subtitle present that will always be below the main title of the passage, although sometimes a subtitle is absent for the passage and not provided by the IELTS.\n",
      "4- Heading and subtitle are always relevant to each other and they describe an overall idea about the text in the main body of the passage.\n",
      "5- Following the heading (and potential subtitle), you'll find the main body of the passage. When you are extracting the text from the main body make sure you keep the format of paragraphs the same as depicted in the picture. for every start of new paragraph insert a '\\n' (line break), so that my parser later understand that this is the start of a new paragraph. There can be a case where a half portion of the last paragraph is in the first page and the other remaining half is at the start of the second page, which means a parapraph might continue to the next page but it is only one paragraph as a whole.\n",
      "6- According to official IELTS website the reading passages have 2000-3000 words in the main body as a whole.\n",
      "\n",
      "Please extract all my mentioned and required details from the image, as I will conduct further analysis on it afterward.\n",
      "\n",
      "Please deeply analyze the full text of the passage and return it in the following JSON format: \n",
      "{\n",
      "    \"instructions\": \"Instruction by IELTS (if present otherwise write None)\",\n",
      "    \"title\": \"Extracted Title Here\",\n",
      "    \"subtitle\": \"Extracted Subtitle Here (if present otherwise write None)\",\n",
      "    \"text\": \"Full text of the passage here.\"\n",
      "\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'ocr_system_prompt.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    sys_prompt = file.read()  \n",
    "    \n",
    "print(sys_prompt)\n",
    "print('-' * 50)\n",
    "\n",
    "file_path = 'ocr_user_prompt.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    user_prompt = file.read()  \n",
    "    \n",
    "print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70429b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from together import Together\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Initialize API client\n",
    "client = Together(base_url=\"https://api.aimlapi.com/v1\", api_key=API_key)\n",
    "\n",
    "# Load image from your local drive\n",
    "# THIS CODE IS NOT BEING USED BELOW\n",
    "image_path = 'reading_test_3_passage_1/stitched_passage.png'  # Replace with your image path\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    # Convert the image to base64 string\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# print(encoded_image)\n",
    "# print(type(encoded_image))\n",
    "# Create the API request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
    "    messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt,\n",
    "            },\n",
    "        \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\":  user_prompt,\n",
    "                    },\n",
    "                    \n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            # stitched passage.\n",
    "                            \"url\" :\"https://drive.google.com/uc?id=1iQQM14fOhw6MOPxexPR03_u9GG7BDXTr\"\n",
    "#                             \"url\": \"https://drive.google.com/uc?id=1XM6uNKcw-kvX0i05D2N-eoHmIonXuTPx\",\n",
    "#                             \"url\": \"https://drive.google.com/uc?id=1GXOZ7spaWSkcvqMwXU6DGwr2IvBiNbjv\",\n",
    "                        },\n",
    "                    },\n",
    "                    \n",
    "                    # IT IS NOT WORKING LIKE THIS, NOT TAKING MULTIPLE IMAGES AT ONCE. (MAYBE I AM MISSING SOMETHING)\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\n",
    "#                             # second page of passage.\n",
    "#                             \"url\": \"https://drive.google.com/uc?id=1GXOZ7spaWSkcvqMwXU6DGwr2IvBiNbjv\",\n",
    "#                         },\n",
    "#                     },\n",
    "                                \n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    # IELTS PASSAGES ARE MOSTLY AROUND 2000-3000 WORDS\n",
    "    max_tokens=3500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e5129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  The extracted JSON data is as follows:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"instructions\": \"You should spend about 20 minutes on Questions 1-13, which are based on Reading Passage 1 below.\",\n",
      "    \"title\": \"Archaeologists discover evidence of prehistoric island settlers\",\n",
      "    \"subtitle\": \"None\",\n",
      "    \"text\": \"In early April 2019, Dr Ceri Shipton and his colleagues from Australian National University became the first archaeologists to explore Obi, one of many tropical islands in Indonesia's Maluku Utara province. The research team's discoveries suggest that the prehistoric people who lived on Obi were adept on both land and sea, hunting in the dense rainforest, foraging on the seashore, and possibly even voyaging between islands.\\n\\nThe excavations were part of a project to learn more about how people first dispersed from mainland Asia, through the Indonesian archipelago and into the prehistoric continent that once connected Australia and New Guinea. The team's earlier research suggested that the northernmost islands in the group, known as the Wallacean islands, including Obi, would have offered the easiest migration route. It also seemed likely that these islands were crucial 'stepping stones' on humans' island-hopping voyages through this region millennia ago. But to support this idea, they needed archaeological evidence for humans living in this remote area in the ancient past. So, they travelled to Obi to look for sites that might reveal evidence of early occupation.\\n\\nJust inland from the village of Kelo on Obi's northern coast, Shipton and his colleagues found two caves containing prehistoric rock shelters that were suitable for excavation. With the permission and help of the local people of Kelo, they dug a small test excavation in each shelter. There they found numerous artefacts, including fragments of axes, some dating to about 14,000 years ago. The earliest axes at Kelo were made using clam shells. Axes made from clam shells from roughly the same time had also previously been found elsewhere in this region, including on the nearby island of Gebe to the northeast. As on Gebe, it is highly likely that Obi's axes were used in the construction of canoes, thus allowing these early peoples to maintain connections between communities on neighbouring islands.\\n\\nThe oldest cultural layers from the Kelo site provided the team with the earliest record for human occupation on Obi, dating back around 14,000 years. At this time the climate was drier and colder than today, and the island's dense rainforests would likely have been much less impenetrable than they are now. Sea levels were about 120 metres lower, meaning Obi was a much larger island, encompassing what is today the separate island of Bisa, as well as several other small islands nearby.\\n\\nRoughly 11,700 years ago, as the most recent ice age ended, the climate became significantly warmer and wetter, no doubt making Obi's jungle much thicker. According to the researchers, it is no coincidence that around this time the first axes crafted from stone rather than sea shells appear, likely in response to their heavy-duty use for clearing and modification of the increasingly dense rainforest. While stone takes about twice as long to grind into an axe compared to shell, the harder material keeps its sharp edge for longer.\\n\\nJudging by the bones which the researchers unearthed in the Kelo caves, people living there mainly hunted the Rothschild's cuscus, a possum-like creature that still lives on Obi today. As the forest grew more dense, people probably used axes to clear patches of forest and make hunting easier.\\n\\nShipton's team's excavation of the shelters at the Kelo site unearthed a volcanic glass substance called obsidian, which must have been brought over from another island, as there is no known source on Obi. It also revealed particular types of beads, similar to those previously found on islands in southern Wallacea. These finds again support the idea that Obi islanders routinely travelled to other islands.\\n\\nThe excavations suggest people successfully lived in the two Kelo shelters for about 10,000 years. But then, about 8,000 years ago, both were abandoned. Did the residents leave Obi completely, or move elsewhere on the island? Perhaps the jungle had grown so thick that axes were no longer a match for the dense undergrowth. Perhaps people simply moved to the coast and turned to fishing rather than hunting as a means of survival.\\n\\nWhatever the reason for the departure, there is no evidence for use of the Kelo shelters after this time, until about 1,000 years ago, when they were re-occupied by people who owned pottery as well as items made out of gold and silver. It seems likely, in view of Obi's location, that this final phase of occupation also saw the Kelo shelters used by people involved in the historic trade in spices between the Maluku islands and the rest of the world.\"\n",
      "}\n",
      "```\n",
      "Assistant response saved to extracted_passage.txt\n"
     ]
    }
   ],
   "source": [
    "# Print the model's response\n",
    "print(\"Assistant: \", response.choices[0].message.content)\n",
    "\n",
    "response_text = response.choices[0].message.content\n",
    "# Write the response to a .txt file\n",
    "file_path = 'extracted_passage.txt'  # Save it as a .txt file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(response_text)  # Write the JSON response as plain text\n",
    "\n",
    "print(f\"Assistant response saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd82fd",
   "metadata": {},
   "source": [
    "# pytesseract code that is NOT working great with the images of passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8aba0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pytesseract\n",
    "\n",
    "# # Load the image\n",
    "# image_path = 'reading_test_3_passage_1/p_1.png'  # Update this path\n",
    "# img = cv2.imread(image_path)\n",
    "\n",
    "# # Convert to grayscale\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Binarize the image (you can adjust the threshold value)\n",
    "# # _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# # Apply Gaussian blur to remove noise\n",
    "# # blurred = cv2.GaussianBlur(binary, (5, 5), 0)\n",
    "\n",
    "# # Perform OCR on the preprocessed image\n",
    "# recognized_text = pytesseract.image_to_string(gray)\n",
    "\n",
    "# # Print the recognized text\n",
    "# print(\"Recognized Text:\")\n",
    "# print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44547b",
   "metadata": {},
   "source": [
    "# TEST: DOWNLOAD THE IMAGE FROM THE DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f37d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # URL of the image you want to download\n",
    "# image_url = \"https://drive.google.com/uc?id=1XM6uNKcw-kvX0i05D2N-eoHmIonXuTPx\"\n",
    "# # Path where you want to save the image\n",
    "# file_path = \"downloaded_image.jpg\"  # Change the filename as needed\n",
    "\n",
    "# # Send a GET request to the image URL\n",
    "# response = requests.get(image_url)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     # Open the file in binary write mode and save the image\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         f.write(response.content)\n",
    "#     print(\"Image downloaded successfully and saved as:\", file_path)\n",
    "# else:\n",
    "#     print(\"Failed to download image. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025803d",
   "metadata": {},
   "source": [
    "# stitch images manually, currently i am sending this image to the llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffae3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# # Load the two images\n",
    "# # image1 = Image.open('reading_test_3_passage_1/p_1.png')\n",
    "# # image2 = Image.open('reading_test_3_passage_1/p_2.png')\n",
    "\n",
    "# image1 = Image.open('book_images/p1.jpg')\n",
    "# image2 = Image.open('book_images/p2.jpg')\n",
    "\n",
    "# # Get the width and height of both images\n",
    "# width1, height1 = image1.size\n",
    "# width2, height2 = image2.size\n",
    "\n",
    "# # Calculate the total height for the new image\n",
    "# total_height = height1 + height2\n",
    "\n",
    "# # Create a new blank image with the same width and total height\n",
    "# new_image = Image.new('RGB', (max(width1, width2), total_height))\n",
    "\n",
    "# # Paste the first image at the top (0, 0)\n",
    "# new_image.paste(image1, (0, 0))\n",
    "\n",
    "# # Paste the second image directly below the first one\n",
    "# new_image.paste(image2, (0, height1))\n",
    "\n",
    "# # Save the stitched image\n",
    "# # new_image.save('reading_test_3_passage_1/stitched_passage.png')\n",
    "# new_image.save('book_images/stitched_passage_book.png')\n",
    "\n",
    "\n",
    "# # Optionally, display the stitched image\n",
    "# # new_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efdfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
